{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "framed-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unique-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    def __init__(self,inp=3):\n",
    "        super(Conv_block,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=inp,out_channels=64,kernel_size=3)\n",
    "        self.conv2a=nn.Conv2d(in_channels=inp,out_channels=64,kernel_size=3,padding=(1,1))\n",
    "        self.conv2b=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3)\n",
    "        self.conv3=nn.Conv2d(in_channels=inp,out_channels=64,kernel_size=3,dilation=2,padding=(1,1))\n",
    "        self.instance_norm=nn.InstanceNorm2d(num_features=64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
    "        self.drop=nn.Dropout2d(p=0.1, inplace=False)\n",
    "        self.pool=nn.MaxPool2d(2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "    \n",
    "    def _forward(self,x):\n",
    "        branch1=self.conv1(x)\n",
    "        branch2=self.conv2a(x)\n",
    "        branch2=self.conv2b(branch2)\n",
    "        branch3=self.conv3(x)\n",
    "        outputs = [branch1,branch2,branch3]\n",
    "        return outputs\n",
    "    \n",
    "    def c_relu(self,x):\n",
    "        x_m=x*-1\n",
    "        ###CHECK WITH DIMENSION AXIS\n",
    "        a=torch.cat([x,x_m],1)\n",
    "        return F.relu(a,inplace=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        outputs=self._forward(x)\n",
    "        for i in outputs:\n",
    "            print(i.shape)\n",
    "        a=torch.cat(outputs,1)\n",
    "        print(a.shape)\n",
    "        a=self.c_relu(a)\n",
    "        print(a.shape)\n",
    "        b=self.instance_norm(a)\n",
    "        print(b.shape)\n",
    "        b=self.drop(b)\n",
    "        print(b.shape)\n",
    "        b=self.pool(b)\n",
    "        print(b.shape)\n",
    "        return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "competitive-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol_Model(nn.Module):\n",
    "    def __init__(self  ):\n",
    "        super(Symbol_Model,self).__init__()\n",
    "        self.conv_block1=Conv_block()\n",
    "        self.conv_block2=Conv_block(384)\n",
    "        self.conv1=nn.Conv2d(in_channels=384,out_channels=97,kernel_size=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv_block1(x)\n",
    "        x=self.conv_block2(x)\n",
    "        x=self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x=x.abs_()\n",
    "        x=x.mean(dim=(2,3))\n",
    "        x=F.sigmoid(x) \n",
    "        print(\"FINAL TENSOR\")\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def loss_fn(output, target):\n",
    "    loss = torch.exp(torch.sum((output - target)**2))\n",
    "    return loss\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = self.loss_fn(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = self.loss_fn(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "entitled-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol_Model(\n",
      "  (conv_block1): Conv_block(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (conv2a): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (conv3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
      "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (drop): Dropout2d(p=0.1, inplace=False)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block2): Conv_block(\n",
      "    (conv1): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (conv2a): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (conv3): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
      "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (drop): Dropout2d(p=0.1, inplace=False)\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=Symbol_Model()\n",
    "# from torchsummary import summary\n",
    "# summary(model, input_size=(3, 128, 128))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "empty-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy\n",
    "image = Image.open(\"/home/saurabhyadav007/Proj/data/ocr/out/imgs/img17f84856-a511-4ca8-88d0-b6e200edf564.jpeg\")\n",
    "image=numpy.array(image)\n",
    "import torchvision\n",
    "image=torchvision.transforms.functional.to_tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "integral-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 170, 89])\n",
      "torch.Size([1, 64, 170, 89])\n",
      "torch.Size([1, 64, 170, 89])\n",
      "torch.Size([1, 192, 170, 89])\n",
      "torch.Size([1, 384, 170, 89])\n",
      "torch.Size([1, 384, 170, 89])\n",
      "torch.Size([1, 384, 170, 89])\n",
      "torch.Size([1, 384, 85, 44])\n",
      "torch.Size([1, 64, 83, 42])\n",
      "torch.Size([1, 64, 83, 42])\n",
      "torch.Size([1, 64, 83, 42])\n",
      "torch.Size([1, 192, 83, 42])\n",
      "torch.Size([1, 384, 83, 42])\n",
      "torch.Size([1, 384, 83, 42])\n",
      "torch.Size([1, 384, 83, 42])\n",
      "torch.Size([1, 384, 41, 21])\n",
      "torch.Size([1, 96, 41, 21])\n",
      "FINAL TENSOR\n",
      "torch.Size([1, 96])\n"
     ]
    }
   ],
   "source": [
    "p=model(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "wicked-header",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5043b050f47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
